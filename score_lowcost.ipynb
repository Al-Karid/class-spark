{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:43:23.749875Z",
     "start_time": "2019-10-30T08:43:20.482817Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.remove(\"metastore_db/db.lck\")\n",
    "    os.remove(\"metastore_db/dbex.lck\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def build_spark_session(app_name, memory='4g', executors=4):\n",
    "    return SparkSession.builder\\\n",
    "                      .appName(app_name)\\\n",
    "                      .config('spark.executor.memory', memory)\\\n",
    "                      .config('spark.executor.instances', executors)\\\n",
    "                      .getOrCreate()\n",
    "\n",
    "spark_session = build_spark_session(app_name='ok-google')\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'objectif est de predire l'appentence des clients a des transport lowcoast.\n",
    "Pour cela, nous utiliserons la librairie Ml de spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perimetre: représente les identifaints des clients accessible à l'étude.\n",
    "histo_client: represente l'historique des données clients sur une période donnée\n",
    "histo_train: represente l'historique des données de commandes trains.\n",
    "histo_lowcost: represente l'historique des données de client lowcost (défini avec le métier).\n",
    "visites: représente l'historique des données de navigation des clients sur le site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - lire les fichiers de données\n",
    "2 - identifier les variables continues et transformer leurs modalités en double.\n",
    "3 - joindre les differentes sources de données en se basant sur les données du périmètre (tous les individus du périmèetre devront apparaitre dans la jointure avec des valeurs NULL si nécessaire pour les colonnes en provenance d'autres sources).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - joindre les dataframe sur la clé ID_CLIENT en concervant tous les clients du périmètre.\n",
    "2 - compter le nombre de ID_CLIENT et vérifier qu'il correspond aux nombre d'ID_CLIENT dans la variable perimètre.\n",
    "3 - Caster les variables continues en double et sauvergarder alors le df obtenu dans le repertoire data sur le cluster.\n",
    "4 - Pour les variables catégorielles, créer une nouvelle variable qui prend la modalité de la variable courante si elle existe et \"NA\" sinon.\n",
    "5- Verifier la cohérence des variables continue. Par exemple pour une variable comme age mettre à -1 tous les ages <0 ou>120ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:45:49.396046Z",
     "start_time": "2019-10-30T08:45:43.153206Z"
    }
   },
   "outputs": [],
   "source": [
    "perimetre = spark_session.read.csv(\"data_clients/sample_perimetre.csv\", header=True)\n",
    "histo_client_raw = spark_session.read.csv(\"data_clients/sample_histo_client.csv\", header=True)\n",
    "histo_train_raw = spark_session.read.csv(\"data_clients/sample_histo_train.csv\", header=True)\n",
    "histo_lowcost_raw = spark_session.read.csv(\"data_clients/sample_histo_lowcost.csv\", header=True)\n",
    "visites_raw = spark_session.read.csv(\"data_clients/sample_visites.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:45:51.626233Z",
     "start_time": "2019-10-30T08:45:51.618651Z"
    }
   },
   "outputs": [],
   "source": [
    "## ecrire une fonction pour transformer les features quantitatives (\"anciennete\", \"recence_cmd\", \"AGE\", etc..) en float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:45:55.031462Z",
     "start_time": "2019-10-30T08:45:54.491822Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "client_cols_to_keep = [\"ID_CLIENT\", 'LBL_STATUT_CLT','LBL_GEO_AIR',\n",
    "            'LBL_SEG_COMPORTEMENTAL','LBL_GEO_TRAIN','LBL_GRP_SEGMENT_NL',\n",
    "            'LBL_SEGMENT_ANTICIPATION','FLG_CMD_CARTE_1225']\n",
    "\n",
    "def cast_columns_of_df(df, cols_to_cast, col_to_keep, cast_type='double'):\n",
    "    \"\"\"cast continuous columns into double since all columns are \"\"\"\n",
    "    return df.select(col_to_keep + [(df[feature].cast('double'))\n",
    "                    for feature in cols_to_cast if 'ID_CLIENT' not in feature])\n",
    "\n",
    "histo_train = cast_columns_of_df(histo_train_raw, histo_train_raw.columns,\n",
    "                                 [\"ID_CLIENT\"], cast_type='double')\n",
    "histo_lowcost = cast_columns_of_df(histo_lowcost_raw, histo_lowcost_raw.columns,\n",
    "                                 [\"ID_CLIENT\"], cast_type='double')\n",
    "\n",
    "visites = cast_columns_of_df(visites_raw, visites_raw.columns,\n",
    "                             [\"ID_CLIENT\"], cast_type='double')\n",
    "\n",
    "histo_client = cast_columns_of_df(histo_client_raw,\n",
    "                                  [\"anciennete\", \"recence_cmd\", \"AGE\"],\n",
    "                                  client_cols_to_keep,\n",
    "                                 cast_type='double')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faire une jointure entre les informations des différentes tables.\n",
    "NB: on conservera tous les clients de la table perimetre.\n",
    "    En effet, ce sont les cleints qu'on souhaite scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combien a t'on de features quatitatives, qualitatives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quelles sont les differentes modalites de la feature LBL_STATUT_CLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quelles sont les features avec valeurs manquantes\n",
    "remplacer les valeurs manquantes par -1 pour toutes les features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:46:21.969151Z",
     "start_time": "2019-10-30T08:46:21.853079Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def input_df(df):\n",
    "    ds = df.select('ID_CLIENT',\n",
    "    f.when(df.LBL_GEO_TRAIN.isin(['Toulouse', 'Lille', 'Dijon',\n",
    "                                  'Lyon', 'Marseille', 'Paris',\n",
    "                                  'Nice', 'Limoges','Rouen','Rennes',\n",
    "                                  'Montpellier', 'Bordeaux', 'Metz',\n",
    "                                  'Strasbourg']), df.LBL_GEO_TRAIN)\\\n",
    "               .otherwise('na').alias('geo_train'),\n",
    "    f.when(df.LBL_GEO_AIR.isin(['Aéroports de Paris Orly',\n",
    "                                'Aéroport de Bâle-Mulhouse / Bassel',\n",
    "                                'Aéroport Lille Lesquin', 'Aéroport de Rennes',\n",
    "                                'Aéroport de Nantes Atlantique',\n",
    "                                'Aéroport de Marseille Provence  (MRS)', \n",
    "                                'Aéroport de Bordeaux Mérignac',\n",
    "                                'Aéroports de Paris Roissy-Charles-de Gaulle', \n",
    "                                \"Aéroport de Nice Côte d'Azur\",\n",
    "                                'Aéroport de Strasbourg',\n",
    "                                'Aéroport de Lyon - Saint Exupéry', \n",
    "                                'Aéroport de Toulouse Blagnac']), df.LBL_GEO_AIR)\\\n",
    "               .otherwise('na').alias('geo_air'),\n",
    "    f.when(df.FLG_CMD_CARTE_1225 == '1', '1')\\\n",
    "                   .otherwise('0').alias('cc_jeunes'),\n",
    "    f.when(df.LBL_STATUT_CLT.isin(['Tres grand', 'Nouveau actif',\n",
    "                                   'Moyen moins', ' Prospect', ' Petit',\n",
    "                                   'Inactif', 'Tres petit',\n",
    "                                   'Nouveau prospect', 'Moyen plus',\n",
    "                                   'Grand']), df.LBL_STATUT_CLT)\\\n",
    "                   .otherwise('na').alias('segt_rfm'),\n",
    "    f.when(df.LBL_SEGMENT_ANTICIPATION.isin(['Peu Anticipateur', 'Tres Anticipateur',\n",
    "                                             'Anticipateur', 'Mixte', 'Non Anticipateur',\n",
    "                                             'Non Defini']), df.LBL_SEGMENT_ANTICIPATION)\\\n",
    "                   .otherwise('na').alias('segt_anticipation'),\n",
    "    f.when(df.LBL_SEG_COMPORTEMENTAL.isin(['Mono-commande',\n",
    "                                           'Comportement Pro',\n",
    "                                           'Exclusifs Agence', \n",
    "                                           'Anticipateurs Methodiques',\n",
    "                                           'Chasseurs Bons Plans', \n",
    "                                           'Rythmes scolaires', 'Nouveaux',\n",
    "                                           'Sans contraintes']),\n",
    "           df.LBL_SEG_COMPORTEMENTAL).otherwise('na').alias('segt_comportemental'), \n",
    "    f.when(df.LBL_GRP_SEGMENT_NL.isin(['Endormi', 'Spectateur', 'Acteur',\n",
    "                                       'Eteint', 'Non defini']),\n",
    "           df.LBL_GRP_SEGMENT_NL).otherwise('na').alias('segt_nl'),\n",
    "    f.when(((df.AGE > 0) & (df.AGE < 100)), df.AGE)\\\n",
    "                   .otherwise(-1).alias('age'),\n",
    "    f.when(df.recence_cmd >= 0, df.recence_cmd)\\\n",
    "                   .otherwise(-1).alias('recence_cmd'),\n",
    "    f.when(((df.mean_duree_voyage > 0) & (df.mean_duree_voyage < 750)),\n",
    "           df.mean_duree_voyage).otherwise(-1).alias('mean_duree_voyage'),\n",
    "    f.when(df.days_since_last_visit >= 0, df.days_since_last_visit)\\\n",
    "                   .otherwise(-1).alias('recence_visite'),\n",
    "    f.when(df.mean_mt_voyage > 0, df.mean_mt_voyage)\\\n",
    "                   .otherwise(-1).alias('mean_mt_voyage'),\n",
    "    f.when(df.anciennete >= 0, df.anciennete)\\\n",
    "                   .otherwise(-1).alias('anciennete'),\n",
    "    f.when(df.nb_od > 0, df.nb_od)\\\n",
    "                   .otherwise(-1).alias('nb_od'),\n",
    "    f.when(df.mean_nb_passagers > 0, df.mean_nb_passagers)\\\n",
    "                   .otherwise(-1).alias('mean_nb_passagers'),\n",
    "    f.when(df.mean_tarif_loisir >= 0, df.mean_tarif_loisir)\\\n",
    "                   .otherwise(-1).alias('mean_tarif_loisir'),\n",
    "    f.when(df.mean_classe_1 >= 0, df.mean_classe_1)\\\n",
    "                   .otherwise(-1).alias('mean_classe_1'),\n",
    "    f.when(df.mean_pointe >= 0, df.mean_pointe)\\\n",
    "                   .otherwise(-1).alias('mean_pointe'),\n",
    "    f.when(df.mean_depart_we >= 0, df.mean_depart_we)\\\n",
    "                   .otherwise(-1).alias('mean_depart_we'),\n",
    "    f.when(df.tx_conversion >= 0, df.tx_conversion)\\\n",
    "                   .otherwise(-1).alias('tx_conversion'),\n",
    "    f.when(df.flg_cmd_lowcost == 1, '1')\\\n",
    "                   .otherwise('0').alias('flg_cmd_lowcost'),\n",
    "    f.when(df.flg_track_nl_lowcost == 1, '1')\\\n",
    "                   .otherwise('0').alias('flg_track_nl_lowcost'), \n",
    "    f.when(df.flg_track_nl == 1, '1')\\\n",
    "                   .otherwise('0').alias('flg_track_nl'))\n",
    "    \n",
    "    return ds\n",
    "#df = input_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles sont les differentes valeurs de notre label : flg_cmd_lowcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:47:26.176433Z",
     "start_time": "2019-10-30T08:47:25.950791Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### features engineering et modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:47:35.868877Z",
     "start_time": "2019-10-30T08:47:35.851878Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocessed_df(df, label=\"flg_cmd_lowcostIndex\"):\n",
    "    max_values_to_define_str_cols = 10\n",
    "    id_col = 'ID_CLIENT'\n",
    "    \n",
    "    dty = dict(df.dtypes)\n",
    "    str_cols = [k for k, v in dty.items() if v == 'string']\n",
    "    str_cols.remove(id_col)\n",
    "    \n",
    "    for c in str_cols:\n",
    "        stringIndexer = StringIndexer(inputCol=c, outputCol=c+\"Index\")\n",
    "        model_str = stringIndexer.fit(df)\n",
    "        df = model_str.transform(df).drop(c)\n",
    "\n",
    "    input_cols = df.columns\n",
    "    input_cols.remove(id_col)\n",
    "    input_cols.remove(label)\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=input_cols,\n",
    "                            outputCol=\"features\")\n",
    "    df = assembler.transform(df)\n",
    "    \n",
    "    featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                   outputCol=\"indexedFeatures\", \n",
    "                   maxCategories=max_values_to_define_str_cols).fit(df)\n",
    "    return featureIndexer.transform(df), df\n",
    "\n",
    "\n",
    "#data, dff = preprocessed_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prelever un sample de data pour notre modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quelle est le label est renseigne pour la modelisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:47:39.766621Z",
     "start_time": "2019-10-30T08:47:39.495207Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = LogisticRegression(labelCol=\"\", \n",
    "                        featuresCol=\"indexedFeatures\",elasticNetParam=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:47:46.200241Z",
     "start_time": "2019-10-30T08:47:46.194669Z"
    }
   },
   "source": [
    "Ajuster le modele de regression logistique et calculer les coefficients de notre modele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluer votre modele courbe roc, precision, rappel, etc...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predire alors les clients lowcoast sur un sample de data n'ayant pas servi à l'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:48:01.633007Z",
     "start_time": "2019-10-30T08:48:01.611551Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(labelCol=\"flg_cmd_lowcostIndex\", \n",
    "                                    featuresCol=\"indexedFeatures\",\n",
    "                                    maxDepth=15, numTrees=100)\n",
    "\n",
    "#model_rf = classifier.fit(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluer les performance de notre modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:48:04.610794Z",
     "start_time": "2019-10-30T08:48:04.406391Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# une autre technique pour evaluer le modele\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluer soit meme le score en calculant le nombre de VP, FP, VN et FN\n",
    "on calculera alors le score qui VP+VN/VP+VN+FP+FN\n",
    "nb: la prediction est automatiquement creee dans le data set et correspond à la colonne prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.6846384667505566, 1.0, 0.9699609990872127)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculer egalement le rappel et la precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:48:18.609975Z",
     "start_time": "2019-10-30T08:48:17.701711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(raw=DenseVector([0.9, 0.1]), label=0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "scoreAndLabels = map(lambda x: (Vectors.dense([1.0 - x[0], x[0]]), x[1]),\n",
    "                     [(0.1, 0.0), (0.1, 1.0), (0.4, 0.0), (0.6, 0.0), (0.6, 1.0), (0.6, 1.0), (0.8, 1.0)])\n",
    "dataset = spark_session.createDataFrame(scoreAndLabels, [\"raw\", \"label\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how to create a spark data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:48:52.572691Z",
     "start_time": "2019-10-30T08:48:52.303613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>Ankit</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>Jalfaizy</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>saurabh</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>Bala</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>2018-07-09</td>\n",
       "      <td>Jules</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43</td>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>Arild</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>sarah</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>Boly</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>2018-04-06</td>\n",
       "      <td>Anita</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>Jules</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-07-24</td>\n",
       "      <td>Soul</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>54</td>\n",
       "      <td>2018-06-17</td>\n",
       "      <td>Gral</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>2018-09-07</td>\n",
       "      <td>Apoh</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>2018-10-04</td>\n",
       "      <td>Dony</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>Tanoh</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>Issouf</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>Bilé</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-05-03</td>\n",
       "      <td>Gagnon</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>28</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>Papiss</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34</td>\n",
       "      <td>2018-02-12</td>\n",
       "      <td>Kravitz</td>\n",
       "      <td>Boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35</td>\n",
       "      <td>2018-05-09</td>\n",
       "      <td>Mouli</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>Jacques</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2018-12-05</td>\n",
       "      <td>soum</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>36</td>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>MBra</td>\n",
       "      <td>Girl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age        date      name   sex\n",
       "0    25  2018-01-03     Ankit  Girl\n",
       "1    22  2018-02-03  Jalfaizy  Girl\n",
       "2    20  2018-01-05   saurabh  Girl\n",
       "3    26  2018-01-12      Bala  Girl\n",
       "4    19  2018-07-09     Jules  Girl\n",
       "5    43  2018-03-18     Arild   Boy\n",
       "6    20  2018-01-05     sarah   Boy\n",
       "7    33  2018-08-12      Boly   Boy\n",
       "8    35  2018-04-06     Anita   Boy\n",
       "9    22  2018-12-06     Jules   Boy\n",
       "10   20  2018-07-24      Soul  Girl\n",
       "11   54  2018-06-17      Gral  Girl\n",
       "12   18  2018-09-07      Apoh  Girl\n",
       "13   32  2018-10-04      Dony  Girl\n",
       "14   31  2018-02-05     Tanoh  Girl\n",
       "15   27  2018-11-12    Issouf   Boy\n",
       "16   29  2018-10-03      Bilé   Boy\n",
       "17   20  2018-05-03    Gagnon   Boy\n",
       "18   28  2018-03-05    Papiss   Boy\n",
       "19   34  2018-02-12   Kravitz   Boy\n",
       "20   35  2018-05-09     Mouli  Girl\n",
       "21   27  2018-08-03   Jacques  Girl\n",
       "22   22  2018-12-05      soum  Girl\n",
       "23   36  2018-04-12      MBra  Girl"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "\n",
    "l = [(datetime.date(2018,1,3), 'Ankit',25, \"Girl\"),\n",
    "     (datetime.date(2018,2,3), 'Jalfaizy',22, \"Girl\"),\n",
    "     (datetime.date(2018,1,5), 'saurabh',20, \"Girl\"),\n",
    "     (datetime.date(2018,1,12), 'Bala',26, \"Girl\"),\n",
    "     (datetime.date(2018,7,9), 'Jules',19, \"Girl\"),\n",
    "     (datetime.date(2018,3,18), 'Arild',43, \"Boy\"),\n",
    "     (datetime.date(2018,1,5), 'sarah',20, \"Boy\"),\n",
    "     (datetime.date(2018,8,12), 'Boly',33, \"Boy\"),\n",
    "     (datetime.date(2018,4,6), 'Anita',35, \"Boy\"),\n",
    "     (datetime.date(2018,12,6), 'Jules',22, \"Boy\"),\n",
    "     (datetime.date(2018,7,24), 'Soul',20, \"Girl\"),\n",
    "     (datetime.date(2018,6,17), 'Gral',54, \"Girl\"),\n",
    "     (datetime.date(2018,9,7), 'Apoh',18, \"Girl\"),\n",
    "     (datetime.date(2018,10,4), 'Dony',32, \"Girl\"),\n",
    "     (datetime.date(2018,2,5), 'Tanoh',31, \"Girl\"),\n",
    "     (datetime.date(2018,11,12), 'Issouf',27, \"Boy\"),\n",
    "     (datetime.date(2018,10,3), 'Bilé',29, \"Boy\"),\n",
    "     (datetime.date(2018,5,3), 'Gagnon',20, \"Boy\"),\n",
    "     (datetime.date(2018,3,5), 'Papiss',28, \"Boy\"),\n",
    "     (datetime.date(2018,2,12), 'Kravitz',34, \"Boy\"),\n",
    "     (datetime.date(2018,5,9), 'Mouli',35, \"Girl\"),\n",
    "     (datetime.date(2018,8,3), 'Jacques',27, \"Girl\"),\n",
    "     (datetime.date(2018,12,5), 'soum',22, \"Girl\"),\n",
    "     (datetime.date(2018,4,12), 'MBra',36, \"Girl\")]\n",
    "\n",
    "rdd = spark_session.sparkContext.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(date=x[0], name=x[1], age=int(x[2]), sex=x[3]))\n",
    "schemaPeople = spark_session.createDataFrame(people)\n",
    "schemaPeople.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- compter le nombre de personne totale \n",
    "2- compter le nombre de fille et de garcon\n",
    "3- quel est l'age moyen, median mini et maxi dans chaque groupe (garcon, fille)\n",
    "4 - quelle est la date de dernière visite de chaque client par rapport à la date d'aujourd'hui (la colonne date correspond à la date de visite)\n",
    "5 - quels sont les personnes qui ont une date de visite < 400 jours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T08:48:59.051276Z",
     "start_time": "2019-10-30T08:48:58.784438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>date_max</th>\n",
       "      <th>days_since_last_visit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>Ankit</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>Jalfaizy</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>saurabh</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>Bala</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>2018-07-09</td>\n",
       "      <td>Jules</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43</td>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>Arild</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>sarah</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>Boly</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35</td>\n",
       "      <td>2018-04-06</td>\n",
       "      <td>Anita</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>Jules</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-07-24</td>\n",
       "      <td>Soul</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>54</td>\n",
       "      <td>2018-06-17</td>\n",
       "      <td>Gral</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>2018-09-07</td>\n",
       "      <td>Apoh</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>2018-10-04</td>\n",
       "      <td>Dony</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>31</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>Tanoh</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>Issouf</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29</td>\n",
       "      <td>2018-10-03</td>\n",
       "      <td>Bilé</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>2018-05-03</td>\n",
       "      <td>Gagnon</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>28</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>Papiss</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34</td>\n",
       "      <td>2018-02-12</td>\n",
       "      <td>Kravitz</td>\n",
       "      <td>Boy</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35</td>\n",
       "      <td>2018-05-09</td>\n",
       "      <td>Mouli</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>Jacques</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2018-12-05</td>\n",
       "      <td>soum</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>36</td>\n",
       "      <td>2018-04-12</td>\n",
       "      <td>MBra</td>\n",
       "      <td>Girl</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age        date      name   sex    date_max  days_since_last_visit\n",
       "0    25  2018-01-03     Ankit  Girl  2019-10-30                    665\n",
       "1    22  2018-02-03  Jalfaizy  Girl  2019-10-30                    634\n",
       "2    20  2018-01-05   saurabh  Girl  2019-10-30                    663\n",
       "3    26  2018-01-12      Bala  Girl  2019-10-30                    656\n",
       "4    19  2018-07-09     Jules  Girl  2019-10-30                    478\n",
       "5    43  2018-03-18     Arild   Boy  2019-10-30                    591\n",
       "6    20  2018-01-05     sarah   Boy  2019-10-30                    663\n",
       "7    33  2018-08-12      Boly   Boy  2019-10-30                    444\n",
       "8    35  2018-04-06     Anita   Boy  2019-10-30                    572\n",
       "9    22  2018-12-06     Jules   Boy  2019-10-30                    328\n",
       "10   20  2018-07-24      Soul  Girl  2019-10-30                    463\n",
       "11   54  2018-06-17      Gral  Girl  2019-10-30                    500\n",
       "12   18  2018-09-07      Apoh  Girl  2019-10-30                    418\n",
       "13   32  2018-10-04      Dony  Girl  2019-10-30                    391\n",
       "14   31  2018-02-05     Tanoh  Girl  2019-10-30                    632\n",
       "15   27  2018-11-12    Issouf   Boy  2019-10-30                    352\n",
       "16   29  2018-10-03      Bilé   Boy  2019-10-30                    392\n",
       "17   20  2018-05-03    Gagnon   Boy  2019-10-30                    545\n",
       "18   28  2018-03-05    Papiss   Boy  2019-10-30                    604\n",
       "19   34  2018-02-12   Kravitz   Boy  2019-10-30                    625\n",
       "20   35  2018-05-09     Mouli  Girl  2019-10-30                    539\n",
       "21   27  2018-08-03   Jacques  Girl  2019-10-30                    453\n",
       "22   22  2018-12-05      soum  Girl  2019-10-30                    329\n",
       "23   36  2018-04-12      MBra  Girl  2019-10-30                    566"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = schemaPeople.select(\"*\",\n",
    "                         f.lit(datetime.date.today()).alias(\"date_max\"))\n",
    "dd.select(\"*\", f.datediff('date_max', 'date')\\\n",
    "                    .alias('days_since_last_visit')).toPandas()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
